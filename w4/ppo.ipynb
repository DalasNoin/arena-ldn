{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from distutils.util import strtobool\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from typing import Any, List, Optional, Union, Tuple, Iterable\n",
    "from einops import rearrange\n",
    "from rl_utils import ppo_parse_args, make_env\n",
    "import tests\n",
    "# import part4_dqn_solution\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "RUNNING_FROM_FILE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    critic: nn.Sequential\n",
    "    actor: nn.Sequential\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv):\n",
    "        super().__init__()\n",
    "        self.obs_shape = envs.observation_space._shape\n",
    "        self.num_actions = 10\n",
    "        self.hidden_size = 64\n",
    "\n",
    "        # todo init layers\n",
    "        final_critic_linear = nn.Linear(self.hidden_size,1)\n",
    "        layer_init(final_critic_linear, std=1)\n",
    "        final_actor_linear = nn.Linear(self.hidden_size,self.num_actions)\n",
    "        layer_init(final_actor_linear, std=0.01)\n",
    "\n",
    "        self.critic = nn.Sequential(nn.Linear(self.obs_shape, self.hidden_size),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Linear(self.hidden_size,self.hidden_size),\n",
    "                                        nn.Tanh(),\n",
    "                                        final_critic_linear)\n",
    "        self.actor = nn.Sequential(nn.Linear(self.obs_shape, self.hidden_size),\n",
    "                                        nn.Tanh(),\n",
    "                                        nn.Linear(64, 64),\n",
    "                                        nn.Tanh(),\n",
    "                                        final_actor_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(\n",
    "    next_value: t.Tensor,\n",
    "    next_done: t.Tensor,\n",
    "    rewards: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    dones: t.Tensor,\n",
    "    device: t.device,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> t.Tensor:\n",
    "    '''Compute advantages using Generalized Advantage Estimation.\n",
    "    \n",
    "    next_value: shape (1, env) - represents V(s_{t+1}) which is needed for the last advantage term\n",
    "    next_done: shape (env,) # log prob of the state the model wanted to transition into\n",
    "    rewards: shape (t, env)\n",
    "    values: shape (t, env)\n",
    "    dones: shape (t, env)\n",
    "\n",
    "    Return: shape (t, env)\n",
    "    '''\n",
    "    next_values = t.zeros_like(values)\n",
    "    next_dones = t.zeros_like(dones)\n",
    "    advantages = t.zeros_like(dones)\n",
    "    next_values = t.cat((values[1:], next_value), dim=0)\n",
    "    next_dones = t.cat((dones[1:], next_done[None]), dim=0)\n",
    "    # shift values and dones by one and concat next value and next done\n",
    "    deltas = rewards + gamma*next_values*(1.0 - next_dones)-values\n",
    "    time_len = advantages.shape[0]\n",
    "    # calculate the advantages from the deltas\n",
    "    last_advantage = t.zeros_like(next_done)\n",
    "    last_done = next_done\n",
    "    for i in range(time_len-1, -1, -1):\n",
    "        last_advantage = deltas[i] + gae_lambda * gamma * (1-last_done) * last_advantage\n",
    "        advantages[i] = last_advantage\n",
    "        last_done = dones[i]\n",
    "    return advantages\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_compute_advantages(compute_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_minibatch_indexes` passed.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Minibatch:\n",
    "    obs: t.Tensor\n",
    "    logprobs: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    advantages: t.Tensor\n",
    "    returns: t.Tensor\n",
    "    values: t.Tensor\n",
    "\n",
    "def minibatch_indexes(batch_size: int, minibatch_size: int, shuffle:bool=False) -> list[np.ndarray]:\n",
    "    '''Return a list of length (batch_size // minibatch_size) where each element is an array of indexes into the batch.\n",
    "\n",
    "    Each index should appear exactly once.\n",
    "    '''\n",
    "    \n",
    "    assert batch_size % minibatch_size == 0\n",
    "    length = batch_size//minibatch_size\n",
    "    indices = np.arange(batch_size, dtype=np.uint32)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    indices = list(indices.reshape((length,minibatch_size)))\n",
    "\n",
    "    return indices\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_minibatch_indexes(minibatch_indexes)\n",
    "\n",
    "def make_minibatches(\n",
    "    obs: t.Tensor,\n",
    "    logprobs: t.Tensor,\n",
    "    actions: t.Tensor,\n",
    "    advantages: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    obs_shape: tuple,\n",
    "    action_shape: tuple,\n",
    "    batch_size: int,\n",
    "    minibatch_size: int,\n",
    ") -> list[Minibatch]:\n",
    "    '''Flatten the environment and steps dimension into one batch dimension, then shuffle and split into minibatches.'''\n",
    "    indices = minibatch_indexes(batch_size=batch_size, minibatch_size=minibatch_size)\n",
    "    returns = advantages + values\n",
    "    minibatches = list()\n",
    "    for idx in indices:\n",
    "        Minibatch(obs=obs[idx].reshape((-1,*obs_shape)), \n",
    "                logprobs=logprobs[idx].reshape((-1,*action_shape)),\n",
    "                actions=actions[idx].reshape((-1,*action_shape)),\n",
    "                advantages=np.flatten(advantages[idx]),\n",
    "                returns=np.flatten(returns[idx]),\n",
    "                values=np.flatten(values[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_policy_loss` passed.\n"
     ]
    }
   ],
   "source": [
    "def calc_policy_loss(\n",
    "    probs: Categorical, mb_action: t.Tensor, mb_advantages: t.Tensor, mb_logprobs: t.Tensor, clip_coef: float\n",
    ") -> t.Tensor:\n",
    "    '''Return the policy loss, suitable for maximisation with gradient ascent.\n",
    "\n",
    "    probs: a distribution containing the actor's unnormalized logits of shape (minibatch, num_actions)\n",
    "\n",
    "    mb_action: action that was actually taken\n",
    "\n",
    "    mb_logprobs: old policy outputs\n",
    "\n",
    "    clip_coef: amount of clipping, denoted by epsilon in Eq 7.\n",
    "    '''\n",
    "    # r denotes the probability ratio between the current policy and the old policy\n",
    "    num_actions = mb_action.shape[0]\n",
    "    idx = t.arange(num_actions)\n",
    "    log_r = probs.logits[idx, mb_action] - mb_logprobs\n",
    "    r = t.exp(log_r)\n",
    "    # normalize\n",
    "    mb_advantages = mb_advantages - t.mean(mb_advantages)\n",
    "    mb_advantages /= t.std(mb_advantages)\n",
    "\n",
    "    # mean(min(r*A, clip(r,1-eps,1+eps)A))\n",
    "    return t.mean(t.min(r*mb_advantages, t.clip(r,1-clip_coef,1+clip_coef)*mb_advantages))\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_policy_loss(calc_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_value_function_loss` passed!\n"
     ]
    }
   ],
   "source": [
    "def calc_value_function_loss(critic: nn.Sequential, mb_obs: t.Tensor, mb_returns: t.Tensor, v_coef: float) -> t.Tensor:\n",
    "    '''Compute the value function portion of the loss function.\n",
    "\n",
    "    v_coef: the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n",
    "    '''\n",
    "    # returns = advantages + values\n",
    "\n",
    "    # 1/2 the mean squared difference between the critic's prediction and the observed returns\n",
    "    # from the paper 1/2*(V_theta-V_targ)**2\n",
    "    V_theta = critic(mb_obs)\n",
    "    return t.mean(1/2*(V_theta-mb_returns)**2) * v_coef\n",
    "\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_value_function_loss(calc_value_function_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy_loss(probs: Categorical, ent_coef: float):\n",
    "    '''Return the entropy loss term.\n",
    "\n",
    "    ent_coef: the coefficient for the entropy loss, which weights its contribution to the overall loss. Denoted by c_2 in the paper.\n",
    "    '''\n",
    "    return probs.entropy().mean() * ent_coef\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_entropy_loss(calc_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOScheduler:\n",
    "    def __init__(self, optimizer, initial_lr: float, end_lr: float, num_updates: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.num_updates = num_updates\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    def step(self):\n",
    "        '''Implement linear learning rate decay so that after num_updates calls to step, the learning rate is end_lr.'''\n",
    "        self.n_step_calls += 1\n",
    "        \n",
    "        r = self.n_step_calls / self.num_updates\n",
    "        lr = self.initial_lr + r * (self.end_lr - self.initial_lr)\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "def make_optimizer(agent: Agent, num_updates: int, initial_lr: float, end_lr: float) -> tuple[optim.Adam, PPOScheduler]:\n",
    "    '''Return an appropriately configured Adam with its attached scheduler.'''\n",
    "    adam = optim.Adam(agent.parameters)\n",
    "    scheduler = PPOScheduler(adam, initial_lr=initial_lr, end_lr=end_lr, num_updates=num_updates)\n",
    "    return adam, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPPOArgs\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     exp_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(\u001b[39m__file__\u001b[39m)\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     seed: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn [35], line 3\u001b[0m, in \u001b[0;36mPPOArgs\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPPOArgs\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     exp_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     seed: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      5\u001b[0m     torch_deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class PPOArgs:\n",
    "    exp_name: str = os.path.basename(__file__).rstrip(\".py\")\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"PPOCart\"\n",
    "    wandb_entity: str = None\n",
    "    capture_video: bool = False\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 500000\n",
    "    learning_rate: float = 0.00025\n",
    "    num_envs: int = 4\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 4\n",
    "    update_epochs: int = 4\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    batch_size: int = 512\n",
    "    minibatch_size: int = 128\n",
    "\n",
    "def train_ppo(args):\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n",
    "    )\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    assert action_shape is not None\n",
    "    assert isinstance(envs.single_action_space, Discrete), \"only discrete action space is supported\"\n",
    "    agent = Agent(envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "    (optimizer, scheduler) = make_optimizer(agent, num_updates, args.learning_rate, 0.0)\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + action_shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    global_step = 0\n",
    "    old_approx_kl = 0.0\n",
    "    approx_kl = 0.0\n",
    "    value_loss = t.tensor(0.0)\n",
    "    policy_loss = t.tensor(0.0)\n",
    "    entropy_loss = t.tensor(0.0)\n",
    "    clipfracs = []\n",
    "    info = []\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    for _ in range(num_updates):\n",
    "        for i in range(0, args.num_steps):\n",
    "            \"YOUR CODE: Rollout phase (see detail #1)\"\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                    break\n",
    "        next_value = rearrange(agent.critic(next_obs), \"env 1 -> 1 env\")\n",
    "        advantages = compute_advantages(\n",
    "            next_value, next_done, rewards, values, dones, device, args.gamma, args.gae_lambda\n",
    "        )\n",
    "        clipfracs.clear()\n",
    "        for _ in range(args.update_epochs):\n",
    "            minibatches = make_minibatches(\n",
    "                obs,\n",
    "                logprobs,\n",
    "                actions,\n",
    "                advantages,\n",
    "                values,\n",
    "                envs.single_observation_space.shape,\n",
    "                action_shape,\n",
    "                args.batch_size,\n",
    "                args.minibatch_size,\n",
    "            )\n",
    "            for mb in minibatches:\n",
    "                \"YOUR CODE: compute loss on the minibatch and step the optimizer (not the scheduler). Do detail #11 (global gradient clipping) here using nn.utils.clip_grad_norm_.\"\n",
    "        scheduler.step()\n",
    "        (y_pred, y_true) = (mb.values.cpu().numpy(), mb.returns.cpu().numpy())\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        with torch.no_grad():\n",
    "            newlogprob: t.Tensor = probs.log_prob(mb.actions)\n",
    "            logratio = newlogprob - mb.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            old_approx_kl = (-logratio).mean().item()\n",
    "            approx_kl = (ratio - 1 - logratio).mean().item()\n",
    "            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        if global_step % 10 == 0:\n",
    "            print(\"steps per second (SPS):\", int(global_step / (time.time() - start_time)))\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "if MAIN:\n",
    "    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "        filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "        print(f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\")\n",
    "        args = PPOArgs()\n",
    "    else:\n",
    "        args = ppo_parse_args()\n",
    "    train_ppo(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39m__file__\u001b[39;49m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((4)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m envs\u001b[39m.\u001b[39msingle_observation_space\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "envs.single_observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
