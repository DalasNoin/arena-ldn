{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import os\n",
    "from typing import Optional, Union, Tuple, List\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import utils\n",
    "\n",
    "Arr = np.ndarray\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "max_episode_steps = 1000\n",
    "IS_CI = os.getenv(\"IS_CI\")\n",
    "N_RUNS = 200 if not IS_CI else 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in utils.py why are solutions imported twice? from w3d5_chapter4_tabular import solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, num_states: int, num_actions: int, start=0, terminal=None):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.start = start\n",
    "        self.terminal = np.array([], dtype=int) if terminal is None else terminal\n",
    "        (self.T, self.R) = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Constructs the T and R tensors from the dynamics of the environment.\n",
    "        Outputs:\n",
    "            T : (num_states, num_actions, num_states) State transition probabilities\n",
    "            R : (num_states, num_actions, num_states) Reward function\n",
    "        '''\n",
    "        num_states = self.num_states\n",
    "        num_actions = self.num_actions\n",
    "        T = np.zeros((num_states, num_actions, num_states))\n",
    "        R = np.zeros((num_states, num_actions, num_states))\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                (states, rewards, probs) = self.dynamics(s, a)\n",
    "                (all_s, all_r, all_p) = self.out_pad(states, rewards, probs)\n",
    "                T[s, a, all_s] = all_p\n",
    "                R[s, a, all_s] = all_r\n",
    "        return (T, R)\n",
    "\n",
    "    def dynamics(self, state: int, action: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "        Computes the distribution over possible outcomes for a given state\n",
    "        and action.\n",
    "        Inputs:\n",
    "            state : int (index of state)\n",
    "            action : int (index of action)\n",
    "        Outputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def render(pi: np.ndarray):\n",
    "        '''\n",
    "        Takes a policy pi, and draws an image of the behavior of that policy, if applicable.\n",
    "        Inputs:\n",
    "            pi : (num_actions,) a policy\n",
    "        Outputs:\n",
    "            None\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def out_pad(self, states: np.ndarray, rewards: np.ndarray, probs: np.ndarray):\n",
    "        '''\n",
    "        Inputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        Outputs:\n",
    "            states  : (num_states,) all the next states\n",
    "            rewards : (num_states,) rewards for each next state transition\n",
    "            probs   : (num_states,) likelihood of each state-reward pair (including zero-prob outcomes.)\n",
    "        '''\n",
    "        out_s = np.arange(self.num_states)\n",
    "        out_r = np.zeros(self.num_states)\n",
    "        out_p = np.zeros(self.num_states)\n",
    "        for i in range(len(states)):\n",
    "            idx = states[i]\n",
    "            out_r[idx] += rewards[i]\n",
    "            out_p[idx] += probs[i]\n",
    "        return (out_s, out_r, out_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy(Environment):\n",
    "    def dynamics(self, state: int, action: int):\n",
    "        (S0, SL, SR) = (0, 1, 2)\n",
    "        LEFT = 0\n",
    "        num_states = 3\n",
    "        num_actions = 2\n",
    "        assert 0 <= state < self.num_states and 0 <= action < self.num_actions\n",
    "        if state == S0:\n",
    "            if action == LEFT:\n",
    "                (next_state, reward) = (SL, 1)\n",
    "            else:\n",
    "                (next_state, reward) = (SR, 0)\n",
    "        elif state == SL:\n",
    "            (next_state, reward) = (S0, 0)\n",
    "        elif state == SR:\n",
    "            (next_state, reward) = (S0, 2)\n",
    "        return (np.array([next_state]), np.array([reward]), np.array([1]))\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAIN:\n",
    "    toy = Toy()\n",
    "    print(toy.T)\n",
    "    print(toy.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norvig(Environment):\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        def state_index(state):\n",
    "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
    "            pos = state[0] + state[1] * self.width\n",
    "            assert 0 <= pos < self.num_states, print(state, pos)\n",
    "            return pos\n",
    "\n",
    "        pos = self.states[state]\n",
    "        move = self.actions[action]\n",
    "        if state in self.terminal or state in self.walls:\n",
    "            return (np.array([state]), np.array([0]), np.array([1]))\n",
    "        out_probs = np.zeros(self.num_actions) + 0.1\n",
    "        out_probs[action] = 0.7\n",
    "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
    "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
    "        new_states = [pos + x for x in self.actions]\n",
    "        for (i, s_new) in enumerate(new_states):\n",
    "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
    "                out_states[i] = state\n",
    "                continue\n",
    "            new_state = state_index(s_new)\n",
    "            if new_state in self.walls:\n",
    "                out_states[i] = state\n",
    "            else:\n",
    "                out_states[i] = new_state\n",
    "            for idx in range(len(self.terminal)):\n",
    "                if new_state == self.terminal[idx]:\n",
    "                    out_rewards[i] = self.goal_rewards[idx]\n",
    "        return (out_states, out_rewards, out_probs)\n",
    "\n",
    "    def render(self, pi: Arr):\n",
    "        assert len(pi) == self.num_states\n",
    "        emoji = [\"â¬†ï¸\", \"âž¡ï¸\", \"â¬‡ï¸\", \"â¬…ï¸\"]\n",
    "        grid = [emoji[act] for act in pi]\n",
    "        grid[3] = \"ðŸŸ©\"\n",
    "        grid[7] = \"ðŸŸ¥\"\n",
    "        grid[5] = \"â¬›\"\n",
    "        print(str(grid[0:4]) + \"\\n\" + str(grid[4:8]) + \"\\n\" + str(grid[8:]))\n",
    "\n",
    "    def __init__(self, penalty=-0.04):\n",
    "        self.height = 3\n",
    "        self.width = 4\n",
    "        self.penalty = penalty\n",
    "        num_states = self.height * self.width\n",
    "        num_actions = 4\n",
    "        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])\n",
    "        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])\n",
    "        self.dim = (self.height, self.width)\n",
    "        terminal = np.array([3, 7], dtype=int)\n",
    "        self.walls = np.array([5], dtype=int)\n",
    "        self.goal_rewards = np.array([1.0, -1])\n",
    "        super().__init__(num_states, num_actions, start=8, terminal=terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-08) -> Arr:\n",
    "    '''\n",
    "    Numerically evaluates the value of a given policy by iterating the Bellman equation\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        pi : shape (num_states,) - The policy to evaluate\n",
    "        gamma: float - Discount factor\n",
    "        eps  : float - Tolerance\n",
    "    Outputs:\n",
    "        value : float (num_states,) - The value function for policy pi\n",
    "    '''\n",
    "    T = env.T\n",
    "    R = env.R\n",
    "    num_states = env.num_states\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    for i in range(1000):\n",
    "        for state in range(num_states):\n",
    "            action = pi[state]\n",
    "            transition_vector = T[state, action]\n",
    "            reward_vector = R[state, action] + gamma*V  # V from the previous iteration is used as the approximation of the value function\n",
    "            V[state] = transition_vector.T @ reward_vector\n",
    "    return V\n",
    "\n",
    "\n",
    "norvig = Norvig()\n",
    "pi = np.random.randint(norvig.num_actions, size=(norvig.num_states,))\n",
    "policy_eval_numerical(env=norvig, pi=pi)\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_numerical, exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:\n",
    "    \"\"\"\n",
    "    Returns the value function for policy and env\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        pi : shape (num_states,) - The policy to evaluate\n",
    "        gamma: float - Discount factor\n",
    "    Outputs:\n",
    "        value : float (num_states,) - The value function for policy pi\n",
    "    \"\"\"\n",
    "    T = env.T\n",
    "    R = env.R\n",
    "    num_states = env.num_states\n",
    "    V = np.zeros(num_states)\n",
    "\n",
    "    # Both of these are (num_states, num_states) matrices. \n",
    "    # P_pi = T(j|pi(i), i) for states i and j\n",
    "    # P_pi = T[:, ]\n",
    "    # reward for states given action \n",
    "    # R_pi = R[:, ]\n",
    "\n",
    "    idx = np.arange(num_states)\n",
    "    P_pi = T[idx, pi[idx]]\n",
    "    R_pi = R[idx, pi[idx]]\n",
    "\n",
    "    r = np.einsum(\"ij,ij -> i\", P_pi, R_pi) \n",
    "\n",
    "    v = np.linalg.inv(np.eye(num_states) - gamma*P_pi) @ r\n",
    "    return v\n",
    "\n",
    "    \n",
    "norvig = Norvig()\n",
    "pi = np.random.randint(norvig.num_actions, size=(norvig.num_states,))\n",
    "v = policy_eval_exact(env=norvig, pi=pi)\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_exact, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        V  : (num_states,) value of each state following some policy pi\n",
    "    Outputs:\n",
    "        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration\n",
    "    '''\n",
    "    num_states = env.num_states\n",
    "    pi = np.random.randint(low=0, high=num_states, size=num_states)\n",
    "    T = env.T\n",
    "    R = env.R\n",
    "\n",
    "    # reward plus value function\n",
    "    Q = R + V\n",
    "    better_policy = np.argmax(np.einsum(\"abc,abc-> ab\", T, Q), axis=1)\n",
    "\n",
    "    return better_policy\n",
    "\n",
    "\n",
    "\n",
    "policy_improvement(env=norvig, V=v)\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_improvement(policy_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_policy(env: Environment, gamma=0.99):\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: environment\n",
    "    Outputs:\n",
    "        pi : (num_states,) int, of actions represeting an optimal policy\n",
    "    '''\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    pi = np.random.randint(low=0, high=num_actions, size=num_states)\n",
    "    V = policy_eval_exact(env, pi, gamma=gamma)\n",
    "    better_pi = policy_improvement(env, V, gamma=gamma)\n",
    "\n",
    "    while np.any(pi != better_pi):\n",
    "        pi = np.copy(better_pi)\n",
    "        V = policy_eval_exact(env, pi, gamma=gamma)\n",
    "        better_pi = policy_improvement(env, V, gamma=gamma)\n",
    "    \n",
    "    return better_pi\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_find_optimal_policy(find_optimal_policy)\n",
    "    penalty = -0.04\n",
    "    norvig = Norvig(penalty)\n",
    "    pi_opt = find_optimal_policy(norvig, gamma=0.99)\n",
    "    norvig.render(pi_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning and DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Temporal Difference learning?\n",
    "\n",
    "In comparison to Monte Carlo methods, TD can update $V_\\pi$ at every timestep. Monte Carlo methods have to wait until the end of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union, List\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.envs.registration\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import utils\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between an observation and a state?\n",
    "\n",
    "An observation is less complete than the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class DiscreteEnviroGym(gym.Env):\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "\n",
    "    def __init__(self, env: Environment):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.observation_space = gym.spaces.Discrete(env.num_states)\n",
    "        self.action_space = gym.spaces.Discrete(env.num_actions)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Samples from the underlying dynamics of the environment\n",
    "        '''\n",
    "        (states, rewards, probs) = self.env.dynamics(self.pos, action)\n",
    "        idx = self.np_random.choice(len(states), p=probs)\n",
    "        (new_state, reward) = (states[idx], rewards[idx])\n",
    "        self.pos = new_state\n",
    "        done = self.pos in self.env.terminal\n",
    "        truncated = False\n",
    "        return (new_state, reward, done, truncated, {\"env\": self.env})\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        self.pos = self.env.start\n",
    "        return (self.pos, {\"env\": self.env}) if return_info else self.pos\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"NorvigGrid-v0\",\n",
    "    entry_point=DiscreteEnviroGym,\n",
    "    max_episode_steps=100,\n",
    "    nondeterministic=True,\n",
    "    kwargs={\"env\": Norvig(penalty=-0.04)},\n",
    ")\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=\"ToyGym-v0\", \n",
    "    entry_point=DiscreteEnviroGym, \n",
    "    max_episode_steps=2, \n",
    "    nondeterministic=False, \n",
    "    kwargs={\"env\": Toy()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experience:\n",
    "    '''A class for storing one piece of experience during an episode run'''\n",
    "    obs: ObsType\n",
    "    act: ActType\n",
    "    reward: float\n",
    "    new_obs: ObsType\n",
    "    new_act: Optional[ActType] = None\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    '''Hyperparameters for agents'''\n",
    "    epsilon: float = 0.1\n",
    "    lr: float = 0.05\n",
    "    optimism: float = 0\n",
    "\n",
    "defaultConfig = AgentConfig()\n",
    "\n",
    "class Agent:\n",
    "    '''Base class for agents interacting with an environment (you do not need to add any implementation here)'''\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma: float = 0.99, seed: int = 0):\n",
    "        self.env = env\n",
    "        self.reset(seed)\n",
    "        self.config = config\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.num_states = env.observation_space.n\n",
    "        self.name = type(self).__name__\n",
    "\n",
    "    def get_action(self, obs: ObsType) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, exp: Experience) -> None:\n",
    "        '''\n",
    "        Agent observes experience, and updates model as appropriate.\n",
    "        Implementation depends on type of agent.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def run_episode(self, seed) -> List[int]:\n",
    "        '''\n",
    "        Simulates one episode of interaction, agent learns as appropriate\n",
    "        Inputs:\n",
    "            seed : Seed for the random number generator\n",
    "        Outputs:\n",
    "            The rewards obtained during the episode\n",
    "        '''\n",
    "        rewards = []\n",
    "        obs = self.env.reset(seed=seed)\n",
    "        self.reset(seed=seed)\n",
    "        done = False\n",
    "        while not done:\n",
    "            act = self.get_action(obs)\n",
    "            (new_obs, reward, done, truncated, info) = self.env.step(act)\n",
    "            done = done or truncated\n",
    "            exp = Experience(obs, act, reward, new_obs)\n",
    "            self.observe(exp)\n",
    "            rewards.append(reward)\n",
    "            obs = new_obs\n",
    "        return rewards\n",
    "\n",
    "    def train(self, n_runs=500):\n",
    "        '''\n",
    "        Run a batch of episodes, and return the total reward obtained per episode\n",
    "        Inputs:\n",
    "            n_runs : The number of episodes to simulate\n",
    "        Outputs:\n",
    "            The discounted sum of rewards obtained for each episode\n",
    "        '''\n",
    "        all_rewards = []\n",
    "        for seed in trange(n_runs):\n",
    "            rewards = self.run_episode(seed)\n",
    "            all_rewards.append(utils.sum_rewards(rewards, self.gamma))\n",
    "        return all_rewards\n",
    "\n",
    "class Random(Agent):\n",
    "    def get_action(self, obs: ObsType) -> ActType:\n",
    "        return self.rng.integers(0, self.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cheater(Agent):\n",
    "    def __init__(self, env: DiscreteEnviroGym, config: AgentConfig = defaultConfig, gamma=0.99, seed=0):\n",
    "        super().__init__(env=env, config=config, gamma=gamma, seed=seed)\n",
    "        \n",
    "        self.optimal_policy = find_optimal_policy(self.env.unwrapped.env)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.optimal_policy[obs]\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    env_toy = gym.make(\"ToyGym-v0\")\n",
    "    agents_toy = [Cheater(env_toy), Random(env_toy)]\n",
    "    for agent in agents_toy:\n",
    "        returns = agent.train(n_runs=100)\n",
    "        plt.plot(utils.cummean(returns), label=agent.name)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Avg. reward on {env_toy.spec.name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
