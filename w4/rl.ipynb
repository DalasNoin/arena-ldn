{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Union, Tuple, List\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "max_episode_steps = 1000\n",
    "IS_CI = os.getenv(\"IS_CI\")\n",
    "N_RUNS = 200 if not IS_CI else 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class MultiArmedBandit(gym.Env):\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "    num_arms: int\n",
    "    stationary: bool\n",
    "    arm_reward_means: np.ndarray\n",
    "    arm_star: int\n",
    "\n",
    "    def __init__(self, num_arms=10, stationary=True):\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.stationary = stationary\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        self.action_space = gym.spaces.Discrete(num_arms)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, arm: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Note: some documentation references a new style which has (termination, truncation) bools in place of the done bool.\n",
    "        '''\n",
    "        assert self.action_space.contains(arm)\n",
    "        if not self.stationary:\n",
    "            q_drift = self.np_random.normal(loc=0.0, scale=0.01, size=self.num_arms)\n",
    "            self.arm_reward_means += q_drift\n",
    "            self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        reward = self.np_random.normal(loc=self.arm_reward_means[arm], scale=1.0)\n",
    "        obs = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        info = dict(best_arm=self.best_arm)\n",
    "        return (obs, reward, done, truncated, info)\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        if self.stationary:\n",
    "            self.arm_reward_means = self.np_random.normal(loc=0.0, scale=1.0, size=self.num_arms)\n",
    "        else:\n",
    "            self.arm_reward_means = np.zeros(shape=[self.num_arms])\n",
    "        self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        if return_info:\n",
    "            return (0, dict())\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\"\n",
    "        bandit_samples = []\n",
    "        for arm in range(self.action_space.n):\n",
    "            bandit_samples += [np.random.normal(loc=self.arm_reward_means[arm], scale=1.0, size=1000)]\n",
    "        plt.violinplot(bandit_samples, showmeans=True)\n",
    "        plt.xlabel(\"Bandit Arm\")\n",
    "        plt.ylabel(\"Reward Distribution\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"ArmedBanditTestbed-v0\",\n",
    "    entry_point=MultiArmedBandit,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    nondeterministic=True,\n",
    "    reward_threshold=1.0,\n",
    "    kwargs={\"num_arms\": 10, \"stationary\": True},\n",
    ")\n",
    "if MAIN:\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\")\n",
    "    print(\"Our env inside its wrappers looks like: \", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''Base class for agents in a multi-armed bandit environment (you do not need to add any implementation here)'''\n",
    "\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        self.num_arms = num_arms\n",
    "        self.reset(seed)\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, action: ActType, reward: float, info: dict) -> None:\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Agent, seed: int):\n",
    "    (rewards, was_best) = ([], [])\n",
    "    env.reset(seed=seed)\n",
    "    agent.reset(seed=seed)\n",
    "    done = False\n",
    "    while not done:\n",
    "        arm = agent.get_action()\n",
    "        (obs, reward, done, truncated, info) = env.step(arm)\n",
    "        done = done or truncated\n",
    "        agent.observe(arm, reward, info)\n",
    "        rewards.append(reward)\n",
    "        was_best.append(1 if arm == info[\"best_arm\"] else 0)\n",
    "    rewards = np.array(rewards, dtype=float)\n",
    "    was_best = np.array(was_best, dtype=int)\n",
    "    return (rewards, was_best)\n",
    "\n",
    "def test_agent(env: gym.Env, agent: Agent, n_runs=200, base_seed=0):\n",
    "    all_rewards = []\n",
    "    all_was_bests = []\n",
    "    for seed in tqdm(range(n_runs), total=n_runs):\n",
    "        (rewards, corrects) = run_episode(env, agent, seed+base_seed)\n",
    "        all_rewards.append(rewards)\n",
    "        all_was_bests.append(corrects)\n",
    "    return (np.array(all_rewards), np.array(all_was_bests))\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def get_action(self) -> ActType:\n",
    "        return self.rng.integers(self.num_arms)\n",
    "\n",
    "if MAIN:\n",
    "    num_arms = 10\n",
    "    stationary = True\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "    random_agent = RandomAgent(num_arms=num_arms, seed=0)\n",
    "    test_agent(env=env, agent=random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(a, n):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot_rewards(\n",
    "    all_rewards: List[np.ndarray], \n",
    "    names: List[str],\n",
    "    moving_avg_window: Optional[int] = 15,\n",
    "):\n",
    "    fig = go.Figure(layout=dict(template=\"simple_white\", title_text=\"Mean reward over all runs\"))\n",
    "    for rewards, name in zip(all_rewards, names):\n",
    "        rewards_avg = rewards.mean(axis=0)\n",
    "        if moving_avg_window is not None:\n",
    "            rewards_avg = moving_avg(rewards_avg, moving_avg_window)\n",
    "        fig.add_trace(go.Scatter(y=rewards_avg, mode=\"lines\", name=name))\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "class RewardAveraging(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int, epsilon: float, optimism: float):\n",
    "        self.epsilon = epsilon\n",
    "        self.optimism = optimism\n",
    "        super().__init__(num_arms=num_arms, seed=seed)\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.integers(self.num_arms)\n",
    "        else:\n",
    "            return np.argmax(self.average_award_by_action)\n",
    "\n",
    "    def observe(self, action: ActType, reward, info):\n",
    "        self.sample_count_by_action[action] += 1\n",
    "        self.average_award_by_action[action] += (reward - self.average_award_by_action[action])/self.sample_count_by_action[action]\n",
    "\n",
    "    def reset(self, seed: int):\n",
    "        super().reset(seed=seed)\n",
    "        self.average_award_by_action = np.zeros(num_arms, dtype=np.float32)\n",
    "        self.sample_count_by_action = np.full(num_arms, fill_value=self.optimism, dtype=np.uint32)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    num_arms = 10\n",
    "    stationary = True\n",
    "    names, all_rewards = [], []\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "\n",
    "    for optimism in [0, 5]:\n",
    "        agent = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=optimism)\n",
    "        (rewards, num_correct) = test_agent(env, agent, n_runs=N_RUNS, base_seed=1)\n",
    "        names.append(str(agent))\n",
    "        all_rewards.append(rewards)\n",
    "        print(agent)\n",
    "        print(f\" -> Frequency of correct arm: {num_correct.mean():.4f}\")\n",
    "        print(f\" -> Average reward: {rewards.mean():.4f}\")\n",
    "\n",
    "    plot_rewards(all_rewards, names, moving_avg_window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheatyMcCheater(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        super().__init__(num_arms=num_arms, seed=seed)\n",
    "        self.best_arm = 0\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        return self.best_arm\n",
    "\n",
    "    def observe(self, action, reward, info):\n",
    "        self.best_arm = info[\"best_arm\"]\n",
    "\n",
    "    def repr(self):\n",
    "        pass\n",
    "\n",
    "if MAIN:\n",
    "    cheater = CheatyMcCheater(num_arms, 0)\n",
    "    reward_averaging = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=0)\n",
    "    random = RandomAgent(num_arms, 0)\n",
    "\n",
    "    names = []\n",
    "    all_rewards = []\n",
    "\n",
    "    for agent in [cheater, reward_averaging, random]:\n",
    "        (rewards, num_correct) = test_agent(env, agent, n_runs=N_RUNS)\n",
    "        names.append(str(agent))\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    plot_rewards(all_rewards, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBActionSelection(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int, c: float):\n",
    "        super().__init__(num_arms=num_arms, seed=seed)\n",
    "        self.c = c  # confidence\n",
    "        self.t = 0\n",
    "\n",
    "    def get_action(self):\n",
    "        return np.argmax(self.average_award_by_action + self.c * np.sqrt(np.log(self.t)/self.sample_count_by_action))\n",
    "\n",
    "    def observe(self, action, reward, info):\n",
    "        self.t += 1\n",
    "        self.sample_count_by_action[action] += 1\n",
    "        self.average_award_by_action[action] += (reward - self.average_award_by_action[action])/self.sample_count_by_action[action]\n",
    "\n",
    "    def reset(self, seed: int):\n",
    "        super().reset(seed=seed)\n",
    "        self.average_award_by_action = np.zeros(num_arms, dtype=np.float32)\n",
    "        # not sure what sutton mean by N(a) == 0 then a is considered a maximizing action\n",
    "        self.sample_count_by_action = np.ones(num_arms, dtype=np.uint32)    \n",
    "\n",
    "if MAIN:\n",
    "    cheater = CheatyMcCheater(num_arms, 0)\n",
    "    reward_averaging = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=0)\n",
    "    reward_averaging_optimism = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=5)\n",
    "    ucb = UCBActionSelection(num_arms, 0, c=2.0)\n",
    "    random = RandomAgent(num_arms, 0)\n",
    "\n",
    "    names = []\n",
    "    all_rewards = []\n",
    "\n",
    "    for agent in [cheater, reward_averaging, reward_averaging_optimism, ucb, random]:\n",
    "        (rewards, num_correct) = test_agent(env, agent, n_runs=N_RUNS, base_seed=1)\n",
    "        names.append(str(agent))\n",
    "        all_rewards.append(rewards)\n",
    "\n",
    "    plot_rewards(all_rewards, names, moving_avg_window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sutton book\n",
    "\n",
    "Q: What is the Markov property?\n",
    "\n",
    "A property of environemnts and their state signals. The state signal must contain all relevant information. Past sensations are summarized compactly in addition to immediate sensations. \n",
    "the reward and state at t+1 can be predicted with the action and state at t, there is no improvement by including information from previous timesteps.\n",
    "\n",
    "Q: What is a Markov decision process?\n",
    "\n",
    "A reinforcement learning task that satisfies the Markov property is called a\n",
    "Markov decision process.\n",
    "\n",
    "Q: Implicitly, we have assumed that the agent need only be Markovian as well. Is this a reasonable assumption?\n",
    "\n",
    "A markovian agents action only depend on the current state. In a Markovian environment this is sufficient. Alternatively in a non markovian environemnt, the agent would require information of past states that is not in the current state to act.\n",
    "\n",
    "Q: Why discount?\n",
    "\n",
    "While it is ok to not use a discount factor for environments with a concise number of timesteps it gets a problem in others. In some environemts there is no terminal step so the sum of rewards could be going to infinity. The sum of (approaching) infinite series is mathematically complex. The discount factor $\\gamma$ means that the agent will prefer rewards sooner rather than later which is often a good property. A \"lazy\" agent that waits around for a long time and then does an action that leads to a reward would be worse than a active agent that gets the same reward straight away. This would also speed up the training process and it allows to approximate things by truncating episodes to a maximum length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proof the recursive function (reward spelt rewward)\n",
    "\n",
    "∀s∈S.V colon instead of period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Is policy $\\pi_L$ better than $\\pi_R$?\n",
    "\n",
    "with $\\gamma$=1 $\\pi_R$ has double the reward but the reward is shifted by one timestep. only if the discount factor is below 0.5 does $\\pi_L$ win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Should the agent go clockwise or anticlockwise around the wall? \n",
    "\n",
    "for small movement penalties the agent should go clockwise since there is a positive reward after 5 steps. also the agent has less of a likelihood of slipping into other cells. for large movement penalties the agent can choose to go to the -1 negative terminal state in 4 steps. the tipping point should be close to movement penalty==-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
