{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m519.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/YaoLu/opt/anaconda3/envs/science/lib/python3.9/site-packages (from gym) (1.23.4)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/YaoLu/opt/anaconda3/envs/science/lib/python3.9/site-packages (from gym) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/YaoLu/opt/anaconda3/envs/science/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.10.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827632 sha256=652ccb9cd1ef2b344c1775c49e87a8b381606bfd0a152bf6b09f1527dc268f0d\n",
      "  Stored in directory: /Users/YaoLu/Library/Caches/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-2.2.0 gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "# ! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Union, Tuple\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "max_episode_steps = 1000\n",
    "IS_CI = os.getenv(\"IS_CI\")\n",
    "N_RUNS = 200 if not IS_CI else 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class MultiArmedBandit(gym.Env):\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "    num_arms: int\n",
    "    stationary: bool\n",
    "    arm_reward_means: np.ndarray\n",
    "    arm_star: int\n",
    "\n",
    "    def __init__(self, num_arms=10, stationary=True):\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.stationary = stationary\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        self.action_space = gym.spaces.Discrete(num_arms)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, arm: ActType) -> Tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Note: some documentation references a new style which has (termination, truncation) bools in place of the done bool.\n",
    "        '''\n",
    "        assert self.action_space.contains(arm)\n",
    "        if not self.stationary:\n",
    "            q_drift = self.np_random.normal(loc=0.0, scale=0.01, size=self.num_arms)\n",
    "            self.arm_reward_means += q_drift\n",
    "            self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        reward = self.np_random.normal(loc=self.arm_reward_means[arm], scale=1.0)\n",
    "        obs = 0\n",
    "        done = False\n",
    "        info = dict(best_arm=self.best_arm)\n",
    "        return (obs, reward, done, info)\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        if self.stationary:\n",
    "            self.arm_reward_means = self.np_random.normal(loc=0.0, scale=1.0, size=self.num_arms)\n",
    "        else:\n",
    "            self.arm_reward_means = np.zeros(shape=[self.num_arms])\n",
    "        self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        if return_info:\n",
    "            return (0, dict())\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\"\n",
    "        bandit_samples = []\n",
    "        for arm in range(self.action_space.n):\n",
    "            bandit_samples += [np.random.normal(loc=self.arm_reward_means[arm], scale=1.0, size=1000)]\n",
    "        plt.violinplot(bandit_samples, showmeans=True)\n",
    "        plt.xlabel(\"Bandit Arm\")\n",
    "        plt.ylabel(\"Reward Distribution\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our env inside its wrappers looks like:  <TimeLimit<OrderEnforcing<PassiveEnvChecker<MultiArmedBandit<ArmedBanditTestbed-v0>>>>>\n"
     ]
    }
   ],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"ArmedBanditTestbed-v0\",\n",
    "    entry_point=MultiArmedBandit,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    nondeterministic=True,\n",
    "    reward_threshold=1.0,\n",
    "    kwargs={\"num_arms\": 10, \"stationary\": True},\n",
    ")\n",
    "if MAIN:\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\")\n",
    "    print(\"Our env inside its wrappers looks like: \", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''Base class for agents in a multi-armed bandit environment (you do not need to add any implementation here)'''\n",
    "\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        self.num_arms = num_arms\n",
    "        self.reset(seed)\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, action: ActType, reward: float, info: dict) -> None:\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Agent, seed: int):\n",
    "    (rewards, was_best) = ([], [])\n",
    "    env.reset(seed=seed)\n",
    "    agent.reset(seed=seed)\n",
    "    done = False\n",
    "    while not done:\n",
    "        arm = agent.get_action()\n",
    "        (obs, reward, done, info) = env.step(arm)\n",
    "        agent.observe(arm, reward, info)\n",
    "        rewards.append(reward)\n",
    "        was_best.append(1 if arm == info[\"best_arm\"] else 0)\n",
    "    rewards = np.array(rewards, dtype=float)\n",
    "    was_best = np.array(was_best, dtype=int)\n",
    "    return (rewards, was_best)\n",
    "\n",
    "def test_agent(env: gym.Env, agent: Agent, n_runs=200):\n",
    "    all_rewards = []\n",
    "    all_was_bests = []\n",
    "    for seed in tqdm(range(n_runs)):\n",
    "        (rewards, corrects) = run_episode(env, agent, seed)\n",
    "        all_rewards.append(rewards)\n",
    "        all_was_bests.append(corrects)\n",
    "    return (np.array(all_rewards), np.array(all_was_bests))\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def get_action(self) -> ActType:\n",
    "        pass\n",
    "\n",
    "if MAIN:\n",
    "    \"TODO: YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(all_rewards: np.ndarray):\n",
    "    (n_runs, n_steps) = all_rewards.shape\n",
    "    (fig, ax) = plt.subplots(figsize=(15, 5))\n",
    "    ax.plot(all_rewards.mean(axis=0), label=\"Mean over all runs\")\n",
    "    quantiles = np.quantile(all_rewards, [0.05, 0.95], axis=0)\n",
    "    ax.fill_between(range(n_steps), quantiles[0], quantiles[1], alpha=0.5)\n",
    "    ax.set(xlabel=\"Step\", ylabel=\"Reward\")\n",
    "    ax.axhline(0, color=\"red\", linewidth=1)\n",
    "    fig.legend()\n",
    "    return fig\n",
    "\n",
    "class RewardAveraging(Agent):\n",
    "    def __init__(self, num_arms: int, seed: int, epsilon: float, optimism: float):\n",
    "        pass\n",
    "\n",
    "    def get_action(self):\n",
    "        pass\n",
    "\n",
    "    def observe(self, action, reward, info):\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int):\n",
    "        pass\n",
    "\n",
    "if MAIN:\n",
    "    env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "    regular_reward_averaging = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=0)\n",
    "    (all_rewards, all_corrects) = test_agent(env, regular_reward_averaging, n_runs=N_RUNS)\n",
    "    print(f\"Frequency of correct arm: {all_corrects.mean()}\")\n",
    "    print(f\"Average reward: {all_rewards.mean()}\")\n",
    "    fig = plot_rewards(all_rewards)\n",
    "    optimistic_reward_averaging = RewardAveraging(num_arms, 0, epsilon=0.1, optimism=5)\n",
    "    (all_rewards, all_corrects) = test_agent(env, optimistic_reward_averaging, n_runs=N_RUNS)\n",
    "    print(f\"Frequency of correct arm: {all_corrects.mean()}\")\n",
    "    print(f\"Average reward: {all_rewards.mean()}\")\n",
    "    plot_rewards(all_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ded5e6f133e31c74d7e61946920be103f96969c2c9abd403ec1a6f8823efeff2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
